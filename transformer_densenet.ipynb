{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import re\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.utils.checkpoint as cp\r\n",
    "from collections import OrderedDict\r\n",
    "from utils import load_state_dict_from_url\r\n",
    "from torch import Tensor\r\n",
    "from typing import Any, List, Tuple\r\n",
    "from torchsummary import summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\r\n",
    "\r\n",
    "model_urls = {\r\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\r\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\r\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\r\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\r\n",
    "}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class _DenseLayer(nn.Module):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        num_input_features: int,\r\n",
    "        growth_rate: int,\r\n",
    "        bn_size: int,\r\n",
    "        drop_rate: float,\r\n",
    "        memory_efficient: bool = False\r\n",
    "    ) -> None:\r\n",
    "        super(_DenseLayer, self).__init__()\r\n",
    "        self.norm1: nn.BatchNorm2d\r\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\r\n",
    "        self.relu1: nn.ReLU\r\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True))\r\n",
    "        self.conv1: nn.Conv2d\r\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\r\n",
    "                                           growth_rate, kernel_size=1, stride=1,\r\n",
    "                                           bias=False)) \r\n",
    "        self.norm2: nn.BatchNorm2d\r\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\r\n",
    "        self.relu2: nn.ReLU\r\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True))\r\n",
    "        self.conv2: nn.Conv2d\r\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\r\n",
    "                                           kernel_size=3, stride=1, padding=1,\r\n",
    "                                           bias=False))\r\n",
    "        self.drop_rate = float(drop_rate)\r\n",
    "        self.memory_efficient = memory_efficient\r\n",
    "\r\n",
    "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\r\n",
    "        concated_features = torch.cat(inputs, 1)\r\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\r\n",
    "        return bottleneck_output\r\n",
    "\r\n",
    "    # todo: rewrite when torchscript supports any\r\n",
    "    def any_requires_grad(self, input: List[Tensor]) -> bool:\r\n",
    "        for tensor in input:\r\n",
    "            if tensor.requires_grad:\r\n",
    "                return True\r\n",
    "        return False\r\n",
    "\r\n",
    "    @torch.jit.unused  # noqa: T484\r\n",
    "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\r\n",
    "        def closure(*inputs):\r\n",
    "            return self.bn_function(inputs)\r\n",
    "\r\n",
    "        return cp.checkpoint(closure, *input)\r\n",
    "\r\n",
    "    @torch.jit._overload_method  # noqa: F811\r\n",
    "    def forward(self, input: List[Tensor]) -> Tensor:\r\n",
    "        pass\r\n",
    "\r\n",
    "    @torch.jit._overload_method  # noqa: F811\r\n",
    "    def forward(self, input: Tensor) -> Tensor:\r\n",
    "        pass\r\n",
    "\r\n",
    "    # torchscript does not yet support *args, so we overload method\r\n",
    "    # allowing it to take either a List[Tensor] or single Tensor\r\n",
    "    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\r\n",
    "        if isinstance(input, Tensor):\r\n",
    "            prev_features = [input]\r\n",
    "        else:\r\n",
    "            prev_features = input\r\n",
    "\r\n",
    "        if self.memory_efficient and self.any_requires_grad(prev_features):\r\n",
    "            if torch.jit.is_scripting():\r\n",
    "                raise Exception(\"Memory Efficient not supported in JIT\")\r\n",
    "\r\n",
    "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\r\n",
    "        else:\r\n",
    "            bottleneck_output = self.bn_function(prev_features)\r\n",
    "\r\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\r\n",
    "        if self.drop_rate > 0:\r\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\r\n",
    "                                     training=self.training)\r\n",
    "        return new_features\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class _DenseBlock(nn.ModuleDict):\r\n",
    "    _version = 2\r\n",
    "\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        num_layers: int,\r\n",
    "        num_input_features: int,\r\n",
    "        bn_size: int,\r\n",
    "        growth_rate: int,\r\n",
    "        drop_rate: float,\r\n",
    "        memory_efficient: bool = False\r\n",
    "    ) -> None:\r\n",
    "        super(_DenseBlock, self).__init__()\r\n",
    "        for i in range(num_layers):\r\n",
    "            layer = _DenseLayer(\r\n",
    "                num_input_features + i * growth_rate,\r\n",
    "                growth_rate=growth_rate,\r\n",
    "                bn_size=bn_size,\r\n",
    "                drop_rate=drop_rate,\r\n",
    "                memory_efficient=memory_efficient,\r\n",
    "            )\r\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\r\n",
    "\r\n",
    "    def forward(self, init_features: Tensor) -> Tensor:\r\n",
    "        features = [init_features]\r\n",
    "        for name, layer in self.items():\r\n",
    "            new_features = layer(features)\r\n",
    "            features.append(new_features)\r\n",
    "        return torch.cat(features, 1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class _Transition(nn.Sequential):\r\n",
    "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\r\n",
    "        super(_Transition, self).__init__()\r\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\r\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\r\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\r\n",
    "                                          kernel_size=1, stride=1, bias=False))\r\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class DenseNet(nn.Module):\r\n",
    "    r\"\"\"Densenet-BC model class, based on\r\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\r\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\r\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\r\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\r\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\r\n",
    "        drop_rate (float) - dropout rate after each dense layer\r\n",
    "        num_classes (int) - number of classification classes\r\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        growth_rate: int = 32,\r\n",
    "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\r\n",
    "        num_init_features: int = 64,\r\n",
    "        bn_size: int = 4,\r\n",
    "        drop_rate: float = 0,\r\n",
    "        num_classes: int = 1000,\r\n",
    "        memory_efficient: bool = False\r\n",
    "    ) -> None:\r\n",
    "\r\n",
    "        super(DenseNet, self).__init__()\r\n",
    "\r\n",
    "        # First convolution\r\n",
    "        self.features = nn.Sequential(OrderedDict([\r\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\r\n",
    "                                padding=3, bias=False)),\r\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\r\n",
    "            ('relu0', nn.ReLU(inplace=True)),\r\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\r\n",
    "        ]))\r\n",
    "\r\n",
    "        # Each denseblock\r\n",
    "        num_features = num_init_features\r\n",
    "        for i, num_layers in enumerate(block_config):\r\n",
    "            block = _DenseBlock(\r\n",
    "                num_layers=num_layers,\r\n",
    "                num_input_features=num_features,\r\n",
    "                bn_size=bn_size,\r\n",
    "                growth_rate=growth_rate,\r\n",
    "                drop_rate=drop_rate,\r\n",
    "                memory_efficient=memory_efficient\r\n",
    "            )\r\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\r\n",
    "            num_features = num_features + num_layers * growth_rate\r\n",
    "            if i != len(block_config) - 1:\r\n",
    "                trans = _Transition(num_input_features=num_features,\r\n",
    "                                    num_output_features=num_features // 2)\r\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\r\n",
    "                num_features = num_features // 2\r\n",
    "\r\n",
    "        # Final batch norm\r\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\r\n",
    "\r\n",
    "        # Linear layer\r\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\r\n",
    "\r\n",
    "        # Official init from torch repo.\r\n",
    "        for m in self.modules():\r\n",
    "            if isinstance(m, nn.Conv2d):\r\n",
    "                nn.init.kaiming_normal_(m.weight)\r\n",
    "            elif isinstance(m, nn.BatchNorm2d):\r\n",
    "                nn.init.constant_(m.weight, 1)\r\n",
    "                nn.init.constant_(m.bias, 0)\r\n",
    "            elif isinstance(m, nn.Linear):\r\n",
    "                nn.init.constant_(m.bias, 0)\r\n",
    "\r\n",
    "    def forward(self, x: Tensor) -> Tensor:\r\n",
    "        features = self.features(x)\r\n",
    "        out = F.relu(features, inplace=True)\r\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\r\n",
    "        out = torch.flatten(out, 1)\r\n",
    "        out = self.classifier(out)\r\n",
    "        return out\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def _load_state_dict(model: nn.Module, model_url: str, progress: bool) -> None:\r\n",
    "    # '.'s are no longer allowed in module names, but previous _DenseLayer\r\n",
    "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\r\n",
    "    # They are also in the checkpoints in model_urls. This pattern is used\r\n",
    "    # to find such keys.\r\n",
    "    pattern = re.compile(\r\n",
    "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\r\n",
    "\r\n",
    "    state_dict = load_state_dict_from_url(model_url, progress=progress)\r\n",
    "    for key in list(state_dict.keys()):\r\n",
    "        res = pattern.match(key)\r\n",
    "        if res:\r\n",
    "            new_key = res.group(1) + res.group(2)\r\n",
    "            state_dict[new_key] = state_dict[key]\r\n",
    "            del state_dict[key]\r\n",
    "    model.load_state_dict(state_dict)\r\n",
    "\r\n",
    "\r\n",
    "def _densenet(\r\n",
    "    arch: str,\r\n",
    "    growth_rate: int,\r\n",
    "    block_config: Tuple[int, int, int, int],\r\n",
    "    num_init_features: int,\r\n",
    "    pretrained: bool,\r\n",
    "    progress: bool,\r\n",
    "    **kwargs: Any\r\n",
    ") -> DenseNet:\r\n",
    "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\r\n",
    "    if pretrained:\r\n",
    "        _load_state_dict(model, model_urls[arch], progress)\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def densenet121(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet:\r\n",
    "    r\"\"\"Densenet-121 model from\r\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
    "    \"\"\"\r\n",
    "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\r\n",
    "                     **kwargs)\r\n",
    "\r\n",
    "\r\n",
    "def densenet161(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet:\r\n",
    "    r\"\"\"Densenet-161 model from\r\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
    "    \"\"\"\r\n",
    "    return _densenet('densenet161', 48, (6, 12, 36, 24), 96, pretrained, progress,\r\n",
    "                     **kwargs)\r\n",
    "\r\n",
    "\r\n",
    "def densenet169(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet:\r\n",
    "    r\"\"\"Densenet-169 model from\r\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
    "    \"\"\"\r\n",
    "    return _densenet('densenet169', 32, (6, 12, 32, 32), 64, pretrained, progress,\r\n",
    "                     **kwargs)\r\n",
    "\r\n",
    "\r\n",
    "def densenet201(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet:\r\n",
    "    r\"\"\"Densenet-201 model from\r\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
    "    \"\"\"\r\n",
    "    return _densenet('densenet201', 32, (6, 12, 48, 32), 64, pretrained, progress,\r\n",
    "                     **kwargs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def test():\r\n",
    "    net = densenet201()\r\n",
    "    summary(net, (3, 224, 224))\r\n",
    "    x = torch.randn((2, 3, 224, 224))\r\n",
    "    y = net(x)\r\n",
    "    print(y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-0e9cfe2a8fd7>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdensenet201\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# make a forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# remove these hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7e3d9e676ffa>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[1;32m<ipython-input-5-b2a36ea88116>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1074\u001b[0m                     \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                     self._forward_hooks.values()):\n\u001b[1;32m-> 1076\u001b[1;33m                 \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1077\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36mhook\u001b[1;34m(module, input, output)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mm_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s-%i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_shape\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_shape\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "75f9e785a93c21ab07cc749eafc89671991f67185f75a7bdd47cef1910e6e74b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}